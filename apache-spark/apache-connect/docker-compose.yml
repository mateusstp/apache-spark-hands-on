version: '3.8'

networks:
  spark-network:
    driver: bridge

services:
  spark-master:
    image: apache-spark-connect:latest
    container_name: spark-master
    networks:
      - spark-network
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_MODE=master
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    command: bash -c '/opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ../data:/data

  spark-worker-1:
    image: apache-spark-connect:latest
    container_name: spark-worker-1
    networks:
      - spark-network
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1g
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 512M
    command: bash -c '/opt/spark/sbin/start-worker.sh spark://spark-master:7077 --memory 1g --cores 1 && tail -f /opt/spark/logs/*'
    volumes:
      - ../data:/data

  # Worker 2 foi removido para economizar recursos

  spark-connect:
    image: apache-spark-connect:latest
    container_name: spark-connect
    networks:
      - spark-network
    depends_on:
      - spark-master
      - spark-worker-1
    ports:
      - "4041:4040" # Mapeia a porta UI do driver do connect para 4041 no host
      - "15002:15002" # Porta do Spark Connect
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 768M
    volumes:
      - ../data:/data # Monta o diretÃ³rio de dados
    # Usa o script oficial, passa --packages via argumentos para bash -c
    command: >
      bash -c '/opt/spark/sbin/start-connect-server.sh "$@" && tail -f /opt/spark/logs/*' _
      --packages org.apache.spark:spark-connect_2.12:3.4.4
